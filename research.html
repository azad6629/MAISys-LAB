<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - MAISys Research Group</title>
    <style>
        :root {
            --primary-color: #003366;
            --secondary-color: #0077be;
            --accent-color: #ffd700;
            --text-color: #333;
            --bg-color: #f9f9f9;
        }
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            color: var(--text-color);
            background-color: var(--bg-color);
        }
        header {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 1rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        nav {
            background-color: var(--secondary-color);
            padding: 0.5rem;
            text-align: center;
        }
        nav ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }
        nav ul li {
            display: inline;
            margin-right: 20px;
        }
        nav ul li a {
            text-decoration: none;
            color: white;
            font-weight: bold;
            transition: color 0.3s ease;
        }
        nav ul li a:hover {
            color: var(--accent-color);
        }
        .container {
            width: 80%;
            margin: auto;
            padding: 20px;
        }
        h2 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--accent-color);
            padding-bottom: 10px;
        }
        .paper-box {
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            margin-bottom: 30px;
            overflow: hidden;
        }
        .paper-info {
            padding: 20px;
        }
        .paper-title {
            font-size: 1.4em;
            color: var(--primary-color);
            margin-bottom: 10px;
        }
        .paper-authors {
            font-style: italic;
            margin-bottom: 10px;
        }
        .paper-publication {
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-link {
            display: inline-block;
            background-color: var(--secondary-color);
            color: white;
            padding: 5px 10px;
            border-radius: 5px;
            text-decoration: none;
            transition: background-color 0.3s ease;
        }
        .paper-link:hover {
            background-color: var(--primary-color);
        }
        .paper-content {
            display: flex;
            flex-wrap: wrap;
            padding: 20px;
            align-items: flex-start;
        }
        .paper-image {
            flex: 0 1 auto;
            max-width: 400px;
            max-height: 300px;
            margin-right: 20px;
            margin-bottom: 20px;
            overflow: hidden;
        }
        .paper-image img {
            width: 100%;
            height: 100%;
            object-fit: contain;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-abstract {
            flex: 1;
            min-width: 300px;
        }
        .abstract-content {
            display: -webkit-box;
            -webkit-line-clamp: 8;
            -webkit-box-orient: vertical;
            overflow: hidden;
            transition: all 0.3s ease;
            text-align: justify;
        }
        .abstract-content.expanded {
            -webkit-line-clamp: unset;
        }
        .read-more {
            color: var(--secondary-color);
            cursor: pointer;
            font-weight: bold;
            display: block;
            margin-top: 10px;
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
        }
        .footer-content {
            display: flex;
            justify-content: space-around;
            align-items: center;
            flex-wrap: wrap;
        }
        .footer-section {
            margin: 10px;
        }
        .social-link {
            color: white;
            font-size: 24px;
            margin-left: 10px;
            transition: color 0.3s ease;
        }
        .social-link:hover {
            color: var(--accent-color);
        }
        @media (max-width: 768px) {
            .paper-image {
                max-width: 100%;
                max-height: none;
                height: auto;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>Medical AI Systems (MAISys)</h1>
        <p>Research Group at Indian Institute of Technology Jodhpur, India</p>
    </header>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="people.html">People</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>
    <div class="container">
        <h2>Our Research</h2>
        
        <div class="paper-box">
            <div class="paper-info">
                <div class="paper-title">MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning</div>
                <div class="paper-authors">Azad Singh, Vandan Gorade, and Deepak Mishra</div>
                <div class="paper-publication">IEEE Journal of Biomedical and Health Informatics, 2024</div>
                <a href="https://ieeexplore.ieee.org/document/10666966" class="paper-link" target="_blank">View Paper</a>
            </div>
            <div class="paper-content">
                <div class="paper-image">
                    <img src="./images/mlvicx.png" alt="MLVICX Architecture">
                </div>
                <div class="paper-abstract">
                    <div class="abstract-content">
                        Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. 
                        By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning. However, for medical images, 
                        like chest X-rays, characterized by complex anatomical structures and diverse clinical conditions, a need arises for representation learning techniques that encode fine-grained 
                        details while preserving the broader contextual information. In this context, we introduce MLVICX (Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning), 
                        an approach to capture rich representations in the form of embeddings from chest X-ray images. Central to our approach is a novel multi-level variance and covariance exploration strategy that effectively 
                        enables the model to detect diagnostically meaningful patterns while reducing redundancy. MLVICX promotes the retention of critical medical insights by adapting global and local contextual details and 
                        enhancing the variance and covariance of the learned embeddings. We demonstrate the performance of MLVICX in advancing self-supervised chest X-ray representation learning through comprehensive experiments. 
                        The performance enhancements we observe across various downstream tasks highlight the significance of the proposed approach in enhancing the utility of chest X-ray embeddings for precision medical diagnosis 
                        and comprehensive image analysis. For pertaining, we used the NIH-Chest X-ray dataset, while for downstream tasks, we utilized NIH-Chest X-ray, Vinbig-CXR, RSNA pneumonia, and SIIM-ACR Pneumothorax datasets. 
                        Overall, we observe up to 3% performance gain over SOTA SSL approaches in various downstream tasks. Additionally, to demonstrate the generalizability of the proposed method, we conducted additional experiments 
                        on fundus images and observed superior performance on multiple datasets.
                    </div>
                    <span class="read-more" onclick="toggleAbstract(this)">Read More</span>
                </div>
            </div>
        </div>

        <div class="paper-box">
            <div class="paper-info">
                <div class="paper-title">CoBooM: Codebook Guided Bootstrapping for Medical Image Representation Learning</div>
                <div class="paper-authors">Azad Singh and Deepak Mishra</div>
                <div class="paper-publication">MICCAI, 2024</div>
                <a href="https://arxiv.org/abs/2408.04262" class="paper-link" target="_blank">View Paper</a>
            </div>
            <div class="paper-content">
                <div class="paper-image">
                    <img src="./images/cobom.png" alt="CoBooM Architecture">
                </div>
                <div class="paper-abstract">
                    <div class="abstract-content">
                    Self-supervised learning (SSL) has emerged as a promising paradigm for medical image analysis by harnessing unannotated data. Despite their potential, the existing SSL approaches overlook the high anatomical similarity inherent in medical images. This makes it challenging for SSL 
                    methods to capture diverse semantic content in medical images consistently. This work introduces a novel and generalized solution that implicitly exploits anatomical similarities by integrating codebooks in SSL. The codebook serves as a concise and informative dictionary of visual 
                    patterns, which not only aids in capturing nuanced anatomical details but also facilitates the creation of robust and generalized feature representations. In this context, we propose CoBooM, a novel framework for self-supervised medical image learning by integrating continuous 
                    and discrete representations. The continuous component ensures the preservation of fine-grained details, while the discrete aspect facilitates coarse-grained feature extraction through the structured embedding space. To understand the effectiveness of CoBooM, we conduct a comprehensive 
                    evaluation of various medical datasets encompassing chest X-rays and fundus images. The experimental results reveal a significant performance gain in classification and segmentation tasks. 
                    </div>
                    <span class="read-more" onclick="toggleAbstract(this)">Read More</span>
                </div>
            </div>
        </div>

        <div class="paper-box"></div>
            <div class="paper-info">
                <div class="paper-title">IDQCE: Instance Discrimination Learning through Quantized Contextual Embeddings for Medical Images</div>
                <div class="paper-authors">Azad Singh and Deepak Mishra</div>
                <div class="paper-publication">ICPR, 2024</div>
                <a href="#" class="paper-link" target="_blank">View Paper</a>
            </div>
            <div class="paper-content">
                <div class="paper-image">
                    <img src="./images/IPID.png" alt="IDQCE Architecture">
                </div>
                <div class="paper-abstract">
                    <div class="abstract-content">
                        Self-supervised pre-training is effective in learning discrim-
                        inative features from unlabeled medical images. However, typical self-
                        supervised models lead to sub-optimal representations due to negligence
                        of high anatomical similarity present in the medical images. This af-
                        fects the negative and positive pairs in discriminative self-supervised
                        models to learn view-invariant representations. Various methods are pro-
                        posed to address this issue. However, many of them either concentrate
                        on preserving pixel-level details or offer solutions for specific modali-
                        ties. In this context, we propose a generalized solution to leverage the
                        anatomical similarities while relaxing the requirements of complex pixel-
                        preservation learning. Specifically, we introduce IDQCE: Instance Dis-
                        crimination Learning through Quantized Contextual Embeddings. The
                        proposed approach leverages the sparse discrete contextual information
                        to guide the self-supervised framework to learn more informative repre-
                        sentations for medical images. We evaluate the representations learned
                        by IDQCE through comprehensive experiments and observe more than
                        3% performance gain under linear evaluation protocol over other SOTA
                        approaches in multiple downstream tasks.
                    </div>
                    <span class="read-more" onclick="toggleAbstract(this)">Read More</span>
                </div>
            </div>
        </div>

        <div class="paper-box"></div>
            <div class="paper-info">
                <div class="paper-title">Large Scale Time-Series Representation Learning via Simultaneous Low- and High-Frequency Feature Bootstrapping</div>
                <div class="paper-authors">Vandan Gorade, Azad Singh (Equal Contribution) and Deepak Mishra </div>
                <div class="paper-publication">IEEE, Transactions on Neural Networks and Learning Systems 2023</div>
                <a href="https://ieeexplore.ieee.org/document/10329573" class="paper-link" target="_blank">View Paper</a>
            </div>
            <div class="paper-content">
                <div class="paper-image">
                    <img src="./images/tnnls.png" alt="tnnls Architecture">
                </div>
                <div class="paper-abstract">
                    <div class="abstract-content">
                        Learning representations from unlabeled time series data is a challenging problem. Most existing self-supervised
                        and unsupervised approaches in the time-series domain fall short in capturing low- and high-frequency features at the same time. As a result, the generalization ability of the learned
                        representations remains limited. Furthermore, some of these methods employ large-scale models like transformers or rely
                        on computationally expensive techniques such as contrastive learning. To tackle these problems, we propose a non-contrastive
                        self-supervised learning approach that efficiently captures low-and high-frequency features in a cost-effective manner. The
                        proposed framework comprises a siamese configuration of a deep neural network with two weight-sharing branches which are
                        followed by low- and high-frequency feature extraction modules. The two branches of the proposed network allow bootstrapping of the latent representation by taking two different augmented
                        views of raw time series data as input. The augmented views are created by applying random transformations sampled from
                        a single set of augmentations. The low- and high-frequency feature extraction modules of the proposed network contain
                        a combination of multilayer perceptron (MLP) and temporal convolutional network (TCN) heads respectively, which capture
                        the temporal dependencies from the raw input data at various scales due to the varying receptive fields. To demonstrate the
                        robustness of our model, we performed extensive experiments and ablation studies on five real-world time-series datasets. Our
                        method achieves state-of-art performance on all the considered datasets.
                    </div>
                    <span class="read-more" onclick="toggleAbstract(this)">Read More</span>
                </div>
            </div>
        </div>

        <!-- Add more paper boxes as needed -->

    </div>
    <footer>
        <div class="footer-content">
            <div class="footer-section">
                <h4>Contact Us</h4>
                <p>Medical AI Systems Group, 217A CSE Department<br>
                Indian Institute of Technology Jodhpur<br>
                NH 62, Nagaur Road, Karwar<br>
                Jodhpur, Rajasthan 342030, India</p>
            </div>
            <div class="footer-section">
                <h4>Get in Touch</h4>
                <p>Email: dmishra@iitj.ac.in<br>
                Phone: +91 291 280 1234</p>
            </div>
            <div class="footer-section">
                <h4>Follow Us</h4>
                <a href="https://www.linkedin.com/company/maisys-iitj" class="social-link" target="_blank">
                    <i class="fab fa-linkedin"></i>
                </a>
            </div>
        </div>
    </footer>

    <script>
        function toggleAbstract(element) {
            const abstractContent = element.previousElementSibling;
            abstractContent.classList.toggle('expanded');
            element.textContent = abstractContent.classList.contains('expanded') ? 'Read Less' : 'Read More';
        }
    </script>
</body>
</html>